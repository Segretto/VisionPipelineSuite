{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb12ba9",
   "metadata": {},
   "source": [
    "# Caderno Jupyter: Estimativa de Profundidade com o Modelo VGGT\n",
    "\n",
    "Este notebook demonstra como usar o modelo VGGT (Vision-Guided Geometry Transformer) para estimar a profundidade de um conjunto de imagens. O processo é dividido em etapas, desde o carregamento dos dados e do modelo até a inferência e o salvamento dos resultados. É um ótimo ponto de partida para entender o fluxo de trabalho prático ao usar um modelo de deep learning pré-treinado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d03dd",
   "metadata": {},
   "source": [
    "## Célula 2: Importação das Bibliotecas\n",
    "\n",
    "Primeiro, vamos importar todas as bibliotecas necessárias.\n",
    "* **`argparse`**: Usado no script original para ler argumentos da linha de comando. No notebook, vamos simular isso para facilitar a alteração dos parâmetros.\n",
    "* **`logging`**: Para exibir mensagens informativas sobre o progresso da execução.\n",
    "* **`pathlib`**: Para manipular caminhos de arquivos e diretórios de forma mais intuitiva.\n",
    "* **`time`**: Para medir o tempo de execução das principais partes do código.\n",
    "* **`numpy`**: Uma biblioteca fundamental para computação numérica em Python, usada aqui para manipular os mapas de profundidade.\n",
    "* **`PIL (Pillow)`**: Usada para salvar as imagens de profundidade em formato PNG.\n",
    "* **`matplotlib.pyplot`**: Para criar e salvar visualizações coloridas dos mapas de profundidade.\n",
    "* **`torch`**: A biblioteca de deep learning que usaremos para carregar e executar o modelo VGGT.\n",
    "* **`vggt`**: A biblioteca específica do modelo que contém a arquitetura do VGGT e funções úteis para pré-processamento de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33bc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/segreto/miniconda3/envs/vggt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/segreto/miniconda3/envs/vggt/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: Importação das Bibliotecas\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80451b",
   "metadata": {},
   "source": [
    "## Célula 4: Funções Utilitárias\n",
    "\n",
    "Nesta seção, definimos algumas funções auxiliares que nos ajudarão ao longo do processo.\n",
    "\n",
    "* **`setup_logger`**: Configura como as mensagens de log (avisos, informações, etc.) serão exibidas.\n",
    "* **`collect_images`**: Procura recursivamente por todos os arquivos de imagem (com extensões como `.png`, `.jpg`) dentro de um diretório e seus subdiretórios.\n",
    "* **`save_color_with_axes_cm`**: Salva o mapa de profundidade como uma imagem colorida. Ela usa um mapa de cores (como \"turbo\") para representar a profundidade e adiciona uma barra de cores com legendas em centímetros, o que facilita a interpretação visual.\n",
    "* **`save_mm_png`**: Salva o mapa de profundidade como uma imagem PNG de 16 bits em tons de cinza. Cada valor de pixel nesta imagem corresponde à profundidade em milímetros. Este formato é ideal para armazenamento preciso dos dados, embora a imagem possa parecer escura em visualizadores comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa79ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Funções Utilitárias\n",
    "\n",
    "# Define as extensões de arquivo de imagem que vamos procurar\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\"}\n",
    "\n",
    "def setup_logger(verbosity: int):\n",
    "    \"\"\"Configura o nível de detalhe das mensagens de log.\"\"\"\n",
    "    level = logging.WARNING if verbosity == 0 else logging.INFO if verbosity == 1 else logging.DEBUG\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    "\n",
    "def collect_images(root: Path) -> list[Path]:\n",
    "    \"\"\"Coleta recursivamente os arquivos de imagem de um diretório.\"\"\"\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Pasta de entrada não encontrada: {root}\")\n",
    "    files = [p for p in sorted(root.rglob(\"*\")) if p.suffix in IMG_EXTS and p.is_file()]\n",
    "    return files\n",
    "\n",
    "def save_color_with_axes_cm(depth_m: np.ndarray, out_path: Path, ticks: int = 9, cmap_name: str = \"turbo\"):\n",
    "    \"\"\"\n",
    "    Salva uma imagem de profundidade colorida com eixos e uma barra de cores em centímetros.\n",
    "    \"\"\"\n",
    "    d_m = depth_m.copy()\n",
    "    # Define um intervalo robusto para a visualização, ignorando valores extremos (outliers)\n",
    "    p1, p99 = np.nanpercentile(d_m, 1), np.nanpercentile(d_m, 99)\n",
    "    if not np.isfinite(p1) or not np.isfinite(p99) or p99 <= p1:\n",
    "        p1, p99 = float(np.nanmin(d_m)), float(np.nanmax(d_m))\n",
    "    if not np.isfinite(p1) or not np.isfinite(p99) or p99 <= p1:\n",
    "        p1, p99 = 0.0, 1.0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5), dpi=150)\n",
    "    im = ax.imshow(d_m, cmap=cmap_name, vmin=p1, vmax=p99, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Profundidade Prevista\")\n",
    "    ax.set_xlabel(\"u (pixels)\")\n",
    "    ax.set_ylabel(\"v (pixels)\")\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Cria marcações na barra de cores em centímetros\n",
    "    ticks_m = np.linspace(p1, p99, num=max(3, ticks))\n",
    "    ticks_cm = ticks_m * 100.0\n",
    "    cbar.set_ticks(ticks_m)\n",
    "    cbar.set_ticklabels([f\"{t:.0f}\" for t in ticks_cm])\n",
    "    cbar.set_label(\"Profundidade (cm)\", rotation=90)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path.as_posix())\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_mm_png(depth_m: np.ndarray, out_path: Path, scale_m: float):\n",
    "    \"\"\"Salva a profundidade como uma imagem PNG de 16 bits em milímetros.\"\"\"\n",
    "    mm = np.clip(depth_m * scale_m * 1000.0, 0, 65535).astype(np.uint16)\n",
    "    Image.fromarray(mm, mode=\"I;16\").save(out_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd213d4",
   "metadata": {},
   "source": [
    "## Célula 6: Configuração dos Parâmetros\n",
    "\n",
    "Em um script Python, os parâmetros são geralmente passados pela linha de comando. Como estamos em um notebook, definimos esses parâmetros diretamente em uma classe `Args`. Isso facilita a experimentação: você pode simplesmente alterar os valores nesta célula e executar o restante do notebook novamente.\n",
    "\n",
    "**Parâmetros importantes:**\n",
    "* `input`: O caminho para a pasta que contém as imagens que você deseja processar.\n",
    "* `output`: O caminho para a pasta onde os resultados serão salvos.\n",
    "* `model_id`: O identificador do modelo pré-treinado a ser usado. O padrão é `\"facebook/VGGT-1B\"`, que será baixado do Hugging Face Hub.\n",
    "* `device`: O dispositivo para executar a inferência. Mude para `\"cpu\"` se você não tiver uma GPU NVIDIA compatível.\n",
    "* `batch_size`: O número de imagens a serem processadas de uma só vez. Um valor maior pode ser mais rápido, mas consome mais memória da GPU. Se você encontrar erros de \"Out of Memory\" (OOM), reduza este valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a414826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Configuração dos Parâmetros\n",
    "\n",
    "class Args:\n",
    "    # !!! MODIFIQUE OS CAMINHOS DE ENTRADA E SAÍDA ABAIXO !!!\n",
    "    input = Path(\"./images\")\n",
    "    output = Path(\"./output\")\n",
    "    \n",
    "    # --- Outros parâmetros ---\n",
    "    # Identificador do modelo no Hugging Face Hub ou caminho local\n",
    "    model_id = \"facebook/VGGT-1B\"\n",
    "    # Dispositivo de inferência ('cuda' para GPU NVIDIA, 'cpu' para CPU)\n",
    "    device = \"cuda\"\n",
    "    # Fator de escala métrica para a profundidade (mantenha 1.0 se desconhecido)\n",
    "    scale_m = 1.0\n",
    "    # Salvar também em formato PNG de 16 bits (milímetros)\n",
    "    save_mm_png = False\n",
    "    # Limitar o número total de imagens a processar (útil para testes rápidos)\n",
    "    max_views = None # Ex: 10 para processar apenas as 10 primeiras imagens\n",
    "    # Tamanho do lote (batch size) para processamento\n",
    "    batch_size = 16\n",
    "    # Mapa de cores para as visualizações\n",
    "    colormap = \"turbo\"\n",
    "    # Número de marcações na barra de cores\n",
    "    cb_ticks = 9\n",
    "    # Nível de verbosidade do log (0: warning, 1: info, 2: debug)\n",
    "    verbose = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Configura o logger e cria a pasta de saída\n",
    "setup_logger(args.verbose)\n",
    "args.output.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23afdc",
   "metadata": {},
   "source": [
    "## Célula 8: Preparação do Ambiente e Carregamento de Dados\n",
    "\n",
    "Nesta etapa, preparamos o ambiente para a inferência:\n",
    "\n",
    "1.  **Seleção do Dispositivo e `dtype`**: O código verifica se uma GPU CUDA está disponível e define o dispositivo (`cuda` ou `cpu`). Ele também seleciona o tipo de dados (`dtype`) para a computação. O uso de `bfloat16` ou `float16` (precisão mista) em GPUs pode acelerar a inferência e reduzir o uso de memória sem uma perda significativa de precisão.\n",
    "2.  **Coleta das Imagens**: A função `collect_images` é chamada para encontrar todos os arquivos de imagem na pasta de entrada definida anteriormente.\n",
    "3.  **Carregamento do Modelo**: O modelo VGGT pré-treinado é baixado e carregado na memória. `.to(device)` move o modelo para a GPU (ou CPU), e `.eval()` o coloca em modo de avaliação, o que desativa camadas como o dropout, que são usadas apenas durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22c81c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:51:55 | INFO     | Dispositivo: cuda | dtype para AMP: torch.bfloat16\n",
      "14:51:55 | INFO     | Coletando imagens...\n",
      "14:51:55 | INFO     | Encontradas 138 imagens.\n",
      "14:51:55 | INFO     | Carregando o modelo: facebook/VGGT-1B\n",
      "14:51:55 | INFO     | using MLP layer as FFN\n"
     ]
    }
   ],
   "source": [
    "# Célula 9: Preparação do Ambiente e Carregamento de Dados\n",
    "\n",
    "# 1. Seleção do Dispositivo e Tipo de Dados (dtype) para Precisão Mista (AMP)\n",
    "device = torch.device(args.device)\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    amp_dtype = torch.bfloat16\n",
    "else:\n",
    "    amp_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "logging.info(f\"Dispositivo: {device} | dtype para AMP: {amp_dtype}\")\n",
    "\n",
    "# 2. Coleta das Imagens\n",
    "logging.info(\"Coletando imagens...\")\n",
    "image_paths = collect_images(args.input)\n",
    "if args.max_views is not None:\n",
    "    image_paths = image_paths[: args.max_views]\n",
    "\n",
    "if not image_paths:\n",
    "    logging.error(f\"Nenhuma imagem encontrada em {args.input}\")\n",
    "else:\n",
    "    logging.info(f\"Encontradas {len(image_paths)} imagens.\")\n",
    "    # Mostra as 5 primeiras imagens como exemplo\n",
    "    for p in image_paths[:5]:\n",
    "        logging.debug(f\"  {p}\")\n",
    "    if len(image_paths) > 5:\n",
    "        logging.debug(\"  ...\")\n",
    "\n",
    "    # 3. Carregamento do Modelo\n",
    "    logging.info(f\"Carregando o modelo: {args.model_id}\")\n",
    "    model = VGGT.from_pretrained(args.model_id).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744be1a8",
   "metadata": {},
   "source": [
    "## Célula 10: Pré-processamento das Imagens\n",
    "\n",
    "Antes de passar as imagens pelo modelo, elas precisam ser pré-processadas. Isso geralmente envolve:\n",
    "* Redimensionar as imagens para o tamanho esperado pelo modelo.\n",
    "* Normalizar os valores dos pixels (por exemplo, para o intervalo [0, 1] e depois subtrair a média e dividir pelo desvio padrão).\n",
    "* Converter as imagens em tensores do PyTorch.\n",
    "\n",
    "A função `load_and_preprocess_images` cuida de todo esse processo para nós, criando um único tensor gigante com todas as imagens prontas para a inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f2aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:52:27 | INFO     | Pré-processando as imagens...\n",
      "14:52:29 | INFO     | Tensor de imagens criado com o formato: (138, 3, 392, 518)\n"
     ]
    }
   ],
   "source": [
    "# Célula 11: Pré-processamento das Imagens\n",
    "\n",
    "if image_paths:\n",
    "    # Converte os caminhos para strings\n",
    "    image_strs = [p.as_posix() for p in image_paths]\n",
    "    \n",
    "    logging.info(\"Pré-processando as imagens...\")\n",
    "    # Carrega e pré-processa todas as imagens, movendo-as para o dispositivo selecionado\n",
    "    images = load_and_preprocess_images(image_strs).to(device)  # Formato: (N, 3, H, W)\n",
    "    logging.info(f\"Tensor de imagens criado com o formato: {tuple(images.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922295f",
   "metadata": {},
   "source": [
    "## Célula 12: Execução da Inferência em Lotes (Batch Processing)\n",
    "\n",
    "Esta é a célula principal onde a \"mágica\" acontece. Para evitar sobrecarregar a memória da GPU, processamos as imagens em lotes (`chunks`).\n",
    "\n",
    "O loop `for` itera sobre o tensor de imagens, pegando um `batch_size` de cada vez. Para cada lote:\n",
    "1.  **`torch.no_grad()`**: Desativa o cálculo de gradientes, o que economiza memória e acelera a computação, já que não estamos treinando o modelo.\n",
    "2.  **`torch.cuda.amp.autocast()`**: Habilita a precisão mista automática, que realiza operações em `float16` ou `bfloat16` sempre que possível para acelerar o processo.\n",
    "3.  **`model.aggregator`**: A primeira parte do modelo VGGT, que extrai e agrega características de múltiplas visualizações (imagens).\n",
    "4.  **`model.depth_head`**: A segunda parte, que usa as características agregadas para prever o mapa de profundidade final.\n",
    "5.  **Salvamento dos Resultados**: Para cada imagem no lote processado, o mapa de profundidade resultante é:\n",
    "    * Convertido para um array NumPy.\n",
    "    * Salvo como um arquivo `.npy` (dados brutos), uma imagem colorida `_color.png` e, opcionalmente, um PNG de 16 bits `_mm.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e35140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:52:31 | INFO     | Processando em lotes (chunks) de 16 imagens para evitar falta de memória (OOM).\n",
      "14:52:31 | INFO     | Processando lote 0000:0016 (16 imagens)\n",
      "14:52:32 | INFO     |   Tempo do Aggregator: 1.003s\n",
      "14:52:32 | INFO     |   Tempo do Depth Head: 0.161s | Total do Lote: 1.165s\n",
      "14:52:32 | INFO     |   Imagem 000 | prof. min/média/max (m): 0.3402/0.8932/1.4779\n",
      "14:52:32 | INFO     |   Imagem 001 | prof. min/média/max (m): 0.2865/0.8903/1.4664\n",
      "14:52:32 | INFO     |   Imagem 002 | prof. min/média/max (m): 0.3272/0.8896/1.4607\n",
      "14:52:32 | INFO     |   Imagem 003 | prof. min/média/max (m): 0.1821/0.8835/1.4693\n",
      "14:52:32 | INFO     |   Imagem 004 | prof. min/média/max (m): 0.2671/0.8873/1.4607\n",
      "14:52:33 | INFO     |   Imagem 005 | prof. min/média/max (m): 0.2302/0.8806/1.4635\n",
      "14:52:33 | INFO     |   Imagem 006 | prof. min/média/max (m): 0.2214/0.8818/1.4635\n",
      "14:52:33 | INFO     |   Imagem 007 | prof. min/média/max (m): 0.3350/0.8827/1.4635\n",
      "14:52:33 | INFO     |   Imagem 008 | prof. min/média/max (m): 0.1632/0.8811/1.4664\n",
      "14:52:33 | INFO     |   Imagem 009 | prof. min/média/max (m): 0.2528/0.8809/1.4550\n",
      "14:52:33 | INFO     |   Imagem 010 | prof. min/média/max (m): 0.2910/0.8793/1.4465\n",
      "14:52:33 | INFO     |   Imagem 011 | prof. min/média/max (m): 0.2979/0.8794/1.4522\n",
      "14:52:34 | INFO     |   Imagem 012 | prof. min/média/max (m): 0.1738/0.8744/1.4380\n",
      "14:52:34 | INFO     |   Imagem 013 | prof. min/média/max (m): 0.1697/0.8803/1.4324\n",
      "14:52:34 | INFO     |   Imagem 014 | prof. min/média/max (m): 0.2821/0.8834/1.4324\n",
      "14:52:34 | INFO     |   Imagem 015 | prof. min/média/max (m): 0.2509/0.8860/1.4493\n",
      "14:52:34 | INFO     | Processando lote 0016:0032 (16 imagens)\n",
      "14:52:35 | INFO     |   Tempo do Aggregator: 0.579s\n",
      "14:52:35 | INFO     |   Tempo do Depth Head: 0.010s | Total do Lote: 0.589s\n",
      "14:52:35 | INFO     |   Imagem 016 | prof. min/média/max (m): 0.3886/0.8962/1.4522\n",
      "14:52:35 | INFO     |   Imagem 017 | prof. min/média/max (m): 0.2799/0.8933/1.4550\n",
      "14:52:35 | INFO     |   Imagem 018 | prof. min/média/max (m): 0.2357/0.8921/1.4578\n",
      "14:52:35 | INFO     |   Imagem 019 | prof. min/média/max (m): 0.3622/0.8910/1.4550\n",
      "14:52:36 | INFO     |   Imagem 020 | prof. min/média/max (m): 0.2910/0.8879/1.4550\n",
      "14:52:36 | INFO     |   Imagem 021 | prof. min/média/max (m): 0.3221/0.8898/1.4493\n",
      "14:52:36 | INFO     |   Imagem 022 | prof. min/média/max (m): 0.3074/0.8907/1.4437\n",
      "14:52:36 | INFO     |   Imagem 023 | prof. min/média/max (m): 0.2320/0.8902/1.4352\n",
      "14:52:36 | INFO     |   Imagem 024 | prof. min/média/max (m): 0.2979/0.8922/1.4437\n",
      "14:52:36 | INFO     |   Imagem 025 | prof. min/média/max (m): 0.3074/0.8930/1.4550\n",
      "14:52:36 | INFO     |   Imagem 026 | prof. min/média/max (m): 0.2979/0.8959/1.4664\n",
      "14:52:37 | INFO     |   Imagem 027 | prof. min/média/max (m): 0.2821/0.8973/1.4664\n",
      "14:52:37 | INFO     |   Imagem 028 | prof. min/média/max (m): 0.2588/0.8966/1.4578\n",
      "14:52:37 | INFO     |   Imagem 029 | prof. min/média/max (m): 0.4136/0.8947/1.4721\n",
      "14:52:37 | INFO     |   Imagem 030 | prof. min/média/max (m): 0.3324/0.8992/1.4635\n",
      "14:52:37 | INFO     |   Imagem 031 | prof. min/média/max (m): 0.2888/0.8949/1.4522\n",
      "14:52:37 | INFO     | Processando lote 0032:0048 (16 imagens)\n",
      "14:52:38 | INFO     |   Tempo do Aggregator: 0.578s\n",
      "14:52:38 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.587s\n",
      "14:52:38 | INFO     |   Imagem 032 | prof. min/média/max (m): 0.2249/0.8982/1.4493\n",
      "14:52:38 | INFO     |   Imagem 033 | prof. min/média/max (m): 0.2000/0.8931/1.4185\n",
      "14:52:38 | INFO     |   Imagem 034 | prof. min/média/max (m): 0.1985/0.8953/1.4296\n",
      "14:52:38 | INFO     |   Imagem 035 | prof. min/média/max (m): 0.2214/0.8969/1.4296\n",
      "14:52:39 | INFO     |   Imagem 036 | prof. min/média/max (m): 0.2470/0.9030/1.4324\n",
      "14:52:39 | INFO     |   Imagem 037 | prof. min/média/max (m): 0.3796/0.9015/1.4324\n",
      "14:52:39 | INFO     |   Imagem 038 | prof. min/média/max (m): 0.3122/0.8970/1.4268\n",
      "14:52:39 | INFO     |   Imagem 039 | prof. min/média/max (m): 0.1711/0.8828/1.4268\n",
      "14:52:39 | INFO     |   Imagem 040 | prof. min/média/max (m): 0.1821/0.8768/1.4213\n",
      "14:52:39 | INFO     |   Imagem 041 | prof. min/média/max (m): 0.1697/0.8723/1.4185\n",
      "14:52:40 | INFO     |   Imagem 042 | prof. min/média/max (m): 0.1570/0.8749/1.4241\n",
      "14:52:40 | INFO     |   Imagem 043 | prof. min/média/max (m): 0.1632/0.8761/1.4185\n",
      "14:52:40 | INFO     |   Imagem 044 | prof. min/média/max (m): 0.1724/0.8740/1.4075\n",
      "14:52:40 | INFO     |   Imagem 045 | prof. min/média/max (m): 0.1879/0.8778/1.4213\n",
      "14:52:40 | INFO     |   Imagem 046 | prof. min/média/max (m): 0.1894/0.8765/1.4185\n",
      "14:52:40 | INFO     |   Imagem 047 | prof. min/média/max (m): 0.1939/0.8807/1.4185\n",
      "14:52:40 | INFO     | Processando lote 0048:0064 (16 imagens)\n",
      "14:52:41 | INFO     |   Tempo do Aggregator: 0.574s\n",
      "14:52:41 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.584s\n",
      "14:52:41 | INFO     |   Imagem 048 | prof. min/média/max (m): 0.1087/0.8736/1.3749\n",
      "14:52:41 | INFO     |   Imagem 049 | prof. min/média/max (m): 0.1006/0.8605/1.3378\n",
      "14:52:41 | INFO     |   Imagem 050 | prof. min/média/max (m): 0.1006/0.8621/1.3404\n",
      "14:52:42 | INFO     |   Imagem 051 | prof. min/média/max (m): 0.1006/0.8636/1.3536\n",
      "14:52:42 | INFO     |   Imagem 052 | prof. min/média/max (m): 0.0990/0.8583/1.3430\n",
      "14:52:42 | INFO     |   Imagem 053 | prof. min/média/max (m): 0.1006/0.8598/1.3378\n",
      "14:52:42 | INFO     |   Imagem 054 | prof. min/média/max (m): 0.0945/0.8582/1.3352\n",
      "14:52:42 | INFO     |   Imagem 055 | prof. min/média/max (m): 0.0902/0.8588/1.3404\n",
      "14:52:42 | INFO     |   Imagem 056 | prof. min/média/max (m): 0.0902/0.8611/1.3378\n",
      "14:52:42 | INFO     |   Imagem 057 | prof. min/média/max (m): 0.0834/0.8595/1.3430\n",
      "14:52:43 | INFO     |   Imagem 058 | prof. min/média/max (m): 0.0834/0.8595/1.3430\n",
      "14:52:43 | INFO     |   Imagem 059 | prof. min/média/max (m): 0.0930/0.8632/1.3562\n",
      "14:52:43 | INFO     |   Imagem 060 | prof. min/média/max (m): 0.0975/0.8611/1.3456\n",
      "14:52:43 | INFO     |   Imagem 061 | prof. min/média/max (m): 0.0960/0.8661/1.3562\n",
      "14:52:43 | INFO     |   Imagem 062 | prof. min/média/max (m): 0.0975/0.8646/1.3589\n",
      "14:52:43 | INFO     |   Imagem 063 | prof. min/média/max (m): 0.0916/0.8680/1.3857\n",
      "14:52:43 | INFO     | Processando lote 0064:0080 (16 imagens)\n",
      "14:52:44 | INFO     |   Tempo do Aggregator: 0.577s\n",
      "14:52:44 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.588s\n",
      "14:52:44 | INFO     |   Imagem 064 | prof. min/média/max (m): 0.2609/0.8662/1.4213\n",
      "14:52:44 | INFO     |   Imagem 065 | prof. min/média/max (m): 0.2180/0.8640/1.4130\n",
      "14:52:44 | INFO     |   Imagem 066 | prof. min/média/max (m): 0.2821/0.8716/1.4130\n",
      "14:52:45 | INFO     |   Imagem 067 | prof. min/média/max (m): 0.1985/0.8711/1.4075\n",
      "14:52:45 | INFO     |   Imagem 068 | prof. min/média/max (m): 0.1879/0.8698/1.4020\n",
      "14:52:45 | INFO     |   Imagem 069 | prof. min/média/max (m): 0.2113/0.8762/1.4102\n",
      "14:52:45 | INFO     |   Imagem 070 | prof. min/média/max (m): 0.3003/0.8751/1.4213\n",
      "14:52:45 | INFO     |   Imagem 071 | prof. min/média/max (m): 0.2691/0.8791/1.4102\n",
      "14:52:46 | INFO     |   Imagem 072 | prof. min/média/max (m): 0.2777/0.8842/1.4075\n",
      "14:52:46 | INFO     |   Imagem 073 | prof. min/média/max (m): 0.2451/0.8854/1.4102\n",
      "14:52:46 | INFO     |   Imagem 074 | prof. min/média/max (m): 0.3171/0.8970/1.3993\n",
      "14:52:46 | INFO     |   Imagem 075 | prof. min/média/max (m): 0.2470/0.8962/1.4047\n",
      "14:52:46 | INFO     |   Imagem 076 | prof. min/média/max (m): 0.2394/0.9061/1.4241\n",
      "14:52:46 | INFO     |   Imagem 077 | prof. min/média/max (m): 0.2163/0.9035/1.4130\n",
      "14:52:47 | INFO     |   Imagem 078 | prof. min/média/max (m): 0.2394/0.9039/1.4130\n",
      "14:52:47 | INFO     |   Imagem 079 | prof. min/média/max (m): 0.2048/0.9008/1.4075\n",
      "14:52:47 | INFO     | Processando lote 0080:0096 (16 imagens)\n",
      "14:52:47 | INFO     |   Tempo do Aggregator: 0.577s\n",
      "14:52:47 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.586s\n",
      "14:52:47 | INFO     |   Imagem 080 | prof. min/média/max (m): 0.1738/0.8715/1.3196\n",
      "14:52:48 | INFO     |   Imagem 081 | prof. min/média/max (m): 0.0902/0.8593/1.3274\n",
      "14:52:48 | INFO     |   Imagem 082 | prof. min/média/max (m): 0.0975/0.8572/1.3145\n",
      "14:52:48 | INFO     |   Imagem 083 | prof. min/média/max (m): 0.0821/0.8511/1.3196\n",
      "14:52:48 | INFO     |   Imagem 084 | prof. min/média/max (m): 0.0724/0.8472/1.3222\n",
      "14:52:48 | INFO     |   Imagem 085 | prof. min/média/max (m): 0.0834/0.8447/1.3170\n",
      "14:52:48 | INFO     |   Imagem 086 | prof. min/média/max (m): 0.0759/0.8428/1.3119\n",
      "14:52:48 | INFO     |   Imagem 087 | prof. min/média/max (m): 0.0610/0.8415/1.3170\n",
      "14:52:49 | INFO     |   Imagem 088 | prof. min/média/max (m): 0.0713/0.8437/1.3196\n",
      "14:52:49 | INFO     |   Imagem 089 | prof. min/média/max (m): 0.0724/0.8444/1.3094\n",
      "14:52:49 | INFO     |   Imagem 090 | prof. min/média/max (m): 0.0702/0.8458/1.3170\n",
      "14:52:49 | INFO     |   Imagem 091 | prof. min/média/max (m): 0.0783/0.8415/1.3145\n",
      "14:52:49 | INFO     |   Imagem 092 | prof. min/média/max (m): 0.0834/0.8559/1.3274\n",
      "14:52:49 | INFO     |   Imagem 093 | prof. min/média/max (m): 0.0808/0.8482/1.3248\n",
      "14:52:50 | INFO     |   Imagem 094 | prof. min/média/max (m): 0.0916/0.8482/1.3222\n",
      "14:52:50 | INFO     |   Imagem 095 | prof. min/média/max (m): 0.0796/0.8399/1.3170\n",
      "14:52:50 | INFO     | Processando lote 0096:0112 (16 imagens)\n",
      "14:52:50 | INFO     |   Tempo do Aggregator: 0.576s\n",
      "14:52:50 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.587s\n",
      "14:52:51 | INFO     |   Imagem 096 | prof. min/média/max (m): 0.1835/0.8779/1.3589\n",
      "14:52:51 | INFO     |   Imagem 097 | prof. min/média/max (m): 0.1054/0.8571/1.3378\n",
      "14:52:51 | INFO     |   Imagem 098 | prof. min/média/max (m): 0.0860/0.8602/1.3536\n",
      "14:52:52 | INFO     |   Imagem 099 | prof. min/média/max (m): 0.0902/0.8587/1.3483\n",
      "14:52:52 | INFO     |   Imagem 100 | prof. min/média/max (m): 0.0902/0.8601/1.3430\n",
      "14:52:52 | INFO     |   Imagem 101 | prof. min/média/max (m): 0.0847/0.8630/1.3589\n",
      "14:52:52 | INFO     |   Imagem 102 | prof. min/média/max (m): 0.0930/0.8618/1.3509\n",
      "14:52:52 | INFO     |   Imagem 103 | prof. min/média/max (m): 0.0874/0.8641/1.3456\n",
      "14:52:52 | INFO     |   Imagem 104 | prof. min/média/max (m): 0.1038/0.8725/1.3509\n",
      "14:52:53 | INFO     |   Imagem 105 | prof. min/média/max (m): 0.1140/0.8724/1.3509\n",
      "14:52:53 | INFO     |   Imagem 106 | prof. min/média/max (m): 0.1006/0.8680/1.3509\n",
      "14:52:53 | INFO     |   Imagem 107 | prof. min/média/max (m): 0.1158/0.8706/1.3456\n",
      "14:52:53 | INFO     |   Imagem 108 | prof. min/média/max (m): 0.1429/0.8727/1.3509\n",
      "14:52:53 | INFO     |   Imagem 109 | prof. min/média/max (m): 0.0874/0.8654/1.3562\n",
      "14:52:53 | INFO     |   Imagem 110 | prof. min/média/max (m): 0.0874/0.8707/1.3536\n",
      "14:52:53 | INFO     |   Imagem 111 | prof. min/média/max (m): 0.1194/0.8736/1.3483\n",
      "14:52:54 | INFO     | Processando lote 0112:0128 (16 imagens)\n",
      "14:52:54 | INFO     |   Tempo do Aggregator: 0.575s\n",
      "14:52:54 | INFO     |   Tempo do Depth Head: 0.009s | Total do Lote: 0.584s\n",
      "14:52:54 | INFO     |   Imagem 112 | prof. min/média/max (m): 0.1396/0.8713/1.3068\n",
      "14:52:54 | INFO     |   Imagem 113 | prof. min/média/max (m): 0.0916/0.8556/1.3094\n",
      "14:52:55 | INFO     |   Imagem 114 | prof. min/média/max (m): 0.0783/0.8578/1.3042\n",
      "14:52:55 | INFO     |   Imagem 115 | prof. min/média/max (m): 0.0796/0.8551/1.2941\n",
      "14:52:55 | INFO     |   Imagem 116 | prof. min/média/max (m): 0.0783/0.8526/1.2916\n",
      "14:52:55 | INFO     |   Imagem 117 | prof. min/média/max (m): 0.0796/0.8531/1.2891\n",
      "14:52:55 | INFO     |   Imagem 118 | prof. min/média/max (m): 0.0771/0.8489/1.2992\n",
      "14:52:55 | INFO     |   Imagem 119 | prof. min/média/max (m): 0.0783/0.8462/1.3119\n",
      "14:52:55 | INFO     |   Imagem 120 | prof. min/média/max (m): 0.0736/0.8410/1.3042\n",
      "14:52:56 | INFO     |   Imagem 121 | prof. min/média/max (m): 0.0821/0.8400/1.3042\n",
      "14:52:56 | INFO     |   Imagem 122 | prof. min/média/max (m): 0.0902/0.8308/1.2966\n",
      "14:52:56 | INFO     |   Imagem 123 | prof. min/média/max (m): 0.0771/0.8306/1.3042\n",
      "14:52:56 | INFO     |   Imagem 124 | prof. min/média/max (m): 0.0808/0.8295/1.3042\n",
      "14:52:56 | INFO     |   Imagem 125 | prof. min/média/max (m): 0.0759/0.8297/1.2941\n",
      "14:52:56 | INFO     |   Imagem 126 | prof. min/média/max (m): 0.0834/0.8321/1.3017\n",
      "14:52:56 | INFO     |   Imagem 127 | prof. min/média/max (m): 0.0847/0.8370/1.3094\n",
      "14:52:57 | INFO     | Processando lote 0128:0138 (10 imagens)\n",
      "14:52:57 | INFO     |   Tempo do Aggregator: 0.312s\n",
      "14:52:57 | INFO     |   Tempo do Depth Head: 0.081s | Total do Lote: 0.394s\n",
      "14:52:57 | INFO     |   Imagem 128 | prof. min/média/max (m): 0.0771/0.8328/1.4324\n",
      "14:52:57 | INFO     |   Imagem 129 | prof. min/média/max (m): 0.0538/0.8190/1.4213\n",
      "14:52:57 | INFO     |   Imagem 130 | prof. min/média/max (m): 0.0582/0.8198/1.4268\n",
      "14:52:57 | INFO     |   Imagem 131 | prof. min/média/max (m): 0.0639/0.8267/1.4352\n",
      "14:52:58 | INFO     |   Imagem 132 | prof. min/média/max (m): 0.0639/0.8259/1.4324\n",
      "14:52:58 | INFO     |   Imagem 133 | prof. min/média/max (m): 0.0582/0.8287/1.4409\n",
      "14:52:58 | INFO     |   Imagem 134 | prof. min/média/max (m): 0.0639/0.8294/1.4465\n",
      "14:52:58 | INFO     |   Imagem 135 | prof. min/média/max (m): 0.0610/0.8302/1.4522\n",
      "14:52:58 | INFO     |   Imagem 136 | prof. min/média/max (m): 0.0601/0.8233/1.4550\n",
      "14:52:58 | INFO     |   Imagem 137 | prof. min/média/max (m): 0.0629/0.8294/1.4578\n",
      "14:52:58 | INFO     | Processamento concluído. 138 imagens processadas em 27.818s\n",
      "14:52:58 | INFO     | Resultados salvos em: output\n"
     ]
    }
   ],
   "source": [
    "# Célula 13: Execução da Inferência em Lotes (Batch Processing)\n",
    "\n",
    "if image_paths:\n",
    "    N_total = images.shape[0]\n",
    "    bs = max(1, args.batch_size)\n",
    "    logging.info(f\"Processando em lotes (chunks) de {bs} imagens para evitar falta de memória (OOM).\")\n",
    "\n",
    "    save_idx = 0\n",
    "    t_all0 = time.perf_counter()\n",
    "\n",
    "    # Itera sobre as imagens em lotes\n",
    "    for start in range(0, N_total, bs):\n",
    "        end = min(start + bs, N_total)\n",
    "        chunk = images[start:end]\n",
    "        n = chunk.shape[0]\n",
    "        logging.info(f\"Processando lote {start:04d}:{end:04d} ({n} imagens)\")\n",
    "\n",
    "        # Contextos de otimização para inferência\n",
    "        with torch.no_grad():\n",
    "            # Adiciona uma dimensão de lote (B=1) para o modelo\n",
    "            images_batched = chunk.unsqueeze(0)  # (B=1, n, 3, H, W)\n",
    "            logging.debug(f\"Formato do lote de entrada: {tuple(images_batched.shape)}\")\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                # 1. Passa as imagens pelo agregador do modelo\n",
    "                aggregated_tokens_list, ps_idx = model.aggregator(images_batched)\n",
    "            t1 = time.perf_counter()\n",
    "            logging.info(f\"  Tempo do Aggregator: {t1 - t0:.3f}s\")\n",
    "\n",
    "            t2 = time.perf_counter()\n",
    "            with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                # 2. Passa os tokens agregados para a cabeça de profundidade\n",
    "                depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images_batched, ps_idx)\n",
    "            t3 = time.perf_counter()\n",
    "            logging.info(f\"  Tempo do Depth Head: {t3 - t2:.3f}s | Total do Lote: {t3 - t0:.3f}s\")\n",
    "\n",
    "        # Garante que o formato do mapa de profundidade está correto\n",
    "        if depth_map.dim() == 5 and depth_map.size(-1) == 1:\n",
    "            depth_map = depth_map.squeeze(-1)\n",
    "        if depth_map.dim() != 4 or depth_map.shape[0] != 1 or depth_map.shape[1] != n:\n",
    "            raise RuntimeError(f\"Formato inesperado do mapa de profundidade: {tuple(depth_map.shape)}\")\n",
    "\n",
    "        # Salva os resultados para cada imagem deste lote\n",
    "        for i in range(n):\n",
    "            d_raw = depth_map[0, i].float().cpu().numpy()  # Profundidade em metros (escala relativa)\n",
    "            d_metric = d_raw * args.scale_m               # Profundidade em metros (escala correta)\n",
    "\n",
    "            d_min = float(np.nanmin(d_metric))\n",
    "            d_mean = float(np.nanmean(d_metric))\n",
    "            d_max = float(np.nanmax(d_metric))\n",
    "            logging.info(f\"  Imagem {save_idx:03d} | prof. min/média/max (m): {d_min:.4f}/{d_mean:.4f}/{d_max:.4f}\")\n",
    "\n",
    "            base = f\"depth_{save_idx:03d}\"\n",
    "            # Salva a profundidade métrica como .npy\n",
    "            np.save(args.output / f\"{base}.npy\", d_metric.astype(np.float32))\n",
    "            # Salva a visualização colorida com barra de cores em cm\n",
    "            save_color_with_axes_cm(d_metric, args.output / f\"{base}_color.png\", ticks=args.cb_ticks, cmap_name=args.colormap)\n",
    "            # Salva opcionalmente o PNG de 16 bits em milímetros\n",
    "            if args.save_mm_png:\n",
    "                save_mm_png(d_raw, args.output / f\"{base}_mm.png\", scale_m=args.scale_m)\n",
    "\n",
    "            save_idx += 1\n",
    "\n",
    "    t_all1 = time.perf_counter()\n",
    "    logging.info(f\"Processamento concluído. {save_idx} imagens processadas em {t_all1 - t_all0:.3f}s\")\n",
    "    logging.info(f\"Resultados salvos em: {args.output.as_posix()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90636aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
